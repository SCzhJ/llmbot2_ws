{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print('Using GPU')\n",
    "  device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from inference.data.test_datasets import LongTestDataset, DAVISTestDataset, YouTubeVOSTestDataset\n",
    "from inference.data.mask_mapper import MaskMapper\n",
    "from model.network import XMem\n",
    "from inference.inference_core import InferenceCore\n",
    "\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# default configuration\n",
    "config = {\n",
    "    'top_k': 30,\n",
    "    'mem_every': 5,\n",
    "    'deep_update_every': -1,\n",
    "    'enable_long_term': True,\n",
    "    'enable_long_term_count_usage': True,\n",
    "    'num_prototypes': 128,\n",
    "    'min_mid_term_frames': 5,\n",
    "    'max_mid_term_frames': 10,\n",
    "    'max_long_term_elements': 10000,\n",
    "}\n",
    "\n",
    "network = XMem(config, '/home/fyp/XMem/saves/XMem.pth').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7 248 249 250 251 252 253 254 255]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(720, 1280)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "mask_name = \"/home/fyp/mask_n.jpg\"\n",
    "mask = np.array(Image.open(mask_name))\n",
    "for i in range(mask.shape[0]):\n",
    "    for j in range(mask.shape[1]):\n",
    "        if mask[i][j] > 200:\n",
    "            mask[i][j] = 255\n",
    "        else:\n",
    "            mask[i][j] = 0\n",
    "print(np.unique(mask))\n",
    "num_objects = len(np.unique(mask)) - 1\n",
    "num_objects\n",
    "print(mask)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "processor = InferenceCore(network, config=config)\n",
    "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
    "frame = cv2.imread(\"/home/fyp/image.jpg\")\n",
    "\n",
    "\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "\n",
    "    # convert numpy array to pytorch tensor format\n",
    "    frame_torch, _ = image_to_torch(frame, device=device)\n",
    "    # initialize with the mask\n",
    "    mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
    "    # the background mask is not fed into the model\n",
    "    prediction = processor.step(frame_torch, mask_torch[1:])\n",
    "    # argmax, convert to numpy\n",
    "    prediction = torch_prob_to_numpy_mask(prediction)\n",
    "\n",
    "frame.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m InferenceCore(network, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m      7\u001b[0m processor\u001b[38;5;241m.\u001b[39mset_all_labels(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_objects\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# consecutive labels\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[43mvideo_name\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# You can change these two numbers\u001b[39;00m\n\u001b[1;32m     11\u001b[0m frames_to_propagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_name' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "processor = InferenceCore(network, config=config)\n",
    "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "\n",
    "# You can change these two numbers\n",
    "frames_to_propagate = 200\n",
    "visualize_every = 20\n",
    "\n",
    "current_frame_index = 0\n",
    "\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "  while (cap.isOpened()):\n",
    "    # load frame-by-frame\n",
    "    _, frame = cap.read()\n",
    "    if frame is None or current_frame_index > frames_to_propagate:\n",
    "      break\n",
    "\n",
    "    # convert numpy array to pytorch tensor format\n",
    "    frame_torch, _ = image_to_torch(frame, device=device)\n",
    "    if current_frame_index == 0:\n",
    "      # initialize with the mask\n",
    "      mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
    "      # the background mask is not fed into the model\n",
    "      prediction = processor.step(frame_torch, mask_torch[1:])\n",
    "    else:\n",
    "      # propagate only\n",
    "      prediction = processor.step(frame_torch)\n",
    "\n",
    "    # argmax, convert to numpy\n",
    "    prediction = torch_prob_to_numpy_mask(prediction)\n",
    "\n",
    "    if current_frame_index % visualize_every == 0:\n",
    "      visualization = overlay_davis(frame, prediction)\n",
    "      display(Image.fromarray(visualization))\n",
    "\n",
    "    current_frame_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
